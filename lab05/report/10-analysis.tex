\section{Аналитический раздел}

\subsection{Гибридные рекомендательные системы}

Гибридные рекомендательные системы сочетают в себе различные методы рекомендаций для достижения лучшей оптимизации системы, избежать некоторые ограничения или проблем, свойственных отдельным рекомендательным моделям. Идея гибридных методов заключается в том, что комбинация алгоритмов обеспечивает более точные и эффективные рекомендации, чем один алгоритм, поскольку недостатки одного алгоритма могут быть преодолены другим алгоритмом. \cite{hybrid}

Термин ``гибридная рекомендательная система'' используется для описания любой рекомендательной системы, которая объединяет несколько методов рекомендаций для получения результата. Сами гибридные системы разделяют на: монолитные, смешанные и ансамбли. Монолитные рекомендаторы берут компоненты различных рекомендаторов и реализуют новый алгоритм. Ансамбль -- это несколько работающих рекомендаторов, результаты работы которых комбинируются в одну рекомендацию. Смешанный рекомендатор возвращает результат работы сразу нескольких рекомендаторов. \cite{hybrid}

\subsection{TF-IDF}

\textbf{TF-IDF} (Term Frequency-Inverse Document Frequency) -- это статистическая мера, используемая в информационном поиске и анализе текста для оценки важности слова в документе относительно всей коллекции документов. Эта мера может быть полезной и в рекомендательных системах для оценки сходства между элементами и пользователями. \cite{tfidf}

\textbf{TF} -- частота слова, отношение числа вхождений некоторого слова к общему числу слов документа, так оценивается важность слова $t_i$ в пределах отдельного документа:

\begin{equation}
	tf(t, d) = \frac{n_t}{\sum_{k}{n_k}},
\end{equation}
\eqexplSetIntro{где}
\begin{eqexpl}[15mm]
\item{$n_t$} число вхождений слова t в документ;
\item{$\sum_{k}{n_k}$} общее количество слов в данном документе.
\end{eqexpl}

\textbf{IDF} -- обратная частота документа, инверсия частоты, с которой некоторое слово встречается в документах коллекции.

\begin{equation}
	IDF(t, D) = log \frac{|D|}{|\{d_i \in D | t \in d_i\}|},
\end{equation}
\eqexplSetIntro{где}
\begin{eqexpl}[15mm]
\item{$|D|$} число документов в коллекции;
\item{$|\{d_i \in D | t \in d_i\}|$} число документов из коллекции $D$, в которых встречается $t$ (когда $n_t \neq 0$).
\end{eqexpl}

Данная мера может быть использована в рекомендательных системах для:

\begin{itemize}
	\item Представления контента, такого как текстовые описания товаров, фильмов или музыкальных треков; каждый объект (например, товар) будет представлен его описанием-вектором, в котором каждое слово представлено его TF-IDF весом, что позволит понимать, какие слова играют важную роль в этом описании -- выделить ``тэги'';
	\item Определения сходства элементов и пользователя через косинусное сходство между векторами; элементы, чьи векторы более похожи на вектор пользователя, могут быть ему рекомендованы;
	\item Улучшения рекомендаций путем подсчета весовых коэффициентов для слов или фраз в профилях пользователей; если пользователь часто взаимодействует с элементами, содержащими определенные ключевые слова, то можно увеличить вес для этих слов в профиле пользователя;
	\item Модификации; TF-IDF может быть использован вместе с другими методами рекомендации, например, с коллаборативной фильтрацией, для улучшения точности и разнообразия рекомендаций.
\end{itemize}

При этом TF-IDF имеет некоторые ограничения: он не учитывает контекст слов и не способен обрабатывать синонимы. \cite{tfidf}

\subsection{Матричная факторизация}
Матричная факторизация -- это класс алгоритмов коллаборативной фильтрации, используемых в рекомендательных системах. Данные алгоритмы работают путем разложения матрицы взаимодействия пользователя с объектами на произведение двух прямоугольных матриц меньшей размерности. Зачастую матричная факторизация используется для улучшения качества персонализированных рекомендаций, позволяя выявить скрытые паттерны и взаимосвязи между пользователями и товарами.\cite{factorization}

Методы матричной факторизации в рекомендательных системах обладают следующими аспектами:

\begin{itemize}
	\item Снижение размерности -- уменьшение объема вычислений и увеличение эффективности;
	\item Скрытые факторы -- данные методы предполагают, что в системе присутствуют некоторые латентные признаки, которые влияют на предпочтения пользователей и характеристики товаров;
	\item Эффективность работы с разреженными данными -- матричная факторизация может эффективно работать с разреженными данными, заполняя недостающие значения.
\end{itemize}

\subsection{Funk SVD}
\textbf{Funk SVD} -- это один из методов матричной факторизации, который был предложен Саймоном Функом и является одним из ранних подходов к коллаборативной фильтрации.

Целью обучения Funk SVD является минимизация разницы между фактическими оценками пользователей и предсказанными на основе разложения матрицы. Для оптимизации параметров разложения и нахождения оптимальных значений скрытых факторов используется градиентный спуск. \cite{svd}

Для оценки качества модели обычно используются среднеквадратичная ошибка (RMSE) и средняя абсолютная ошибка (MAE).

Funk SVD имеет также и свои ограничения -- он не способен учитывать неявные обратные связи и у него отсутствует возможность холодного старта.

Прогнозируемую оценку можно рассчитать как:

\begin{equation}
	\widetilde{R} = HW
\end{equation}
\eqexplSetIntro{где}
\begin{eqexpl}[35mm]
\item{$\widetilde{R} \in \mathbb{R}^{users \times items}$} матрица оценок пользователя;
\item{$H \in \mathbb{R}^{userx \times latent factors}$} содержит латентные признаки пользователя;
\item{$W \in \mathbb{R}^{latent factors \times items}$} скрытые признаки объекта.
\end{eqexpl}

В частности, прогнозируемая оценка пользователя $u$ объекту $i$:
\begin{equation}
	\widetilde{r}_{ui} = \sum^{n factors}_{f = 0} H_{u, f} W_{f, i}
\end{equation}

\pagebreak